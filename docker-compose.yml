# Game AI 客服系统 - Docker Compose 统一配置
#
# 使用方式:
#   启动核心服务:     docker-compose up -d
#   启动全部服务:     docker-compose --profile monitoring up -d
#   停止所有服务:     docker-compose --profile monitoring down
#   查看服务状态:     docker-compose --profile monitoring ps

services:
  # ==================== 核心服务 ====================

  postgres:
    image: postgres:14-alpine
    container_name: game-ai-cs-postgres
    environment:
      POSTGRES_DB: game_ai_cs
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    ports:
      - "22101:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d game_ai_cs"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - game-ai-cs-network
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: game-ai-cs-redis
    ports:
      - "22102:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    networks:
      - game-ai-cs-network
    restart: unless-stopped

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: game-ai-cs-backend
    environment:
      - NODE_ENV=production
      - PORT=21101
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LOG_FORMAT=${LOG_FORMAT:-json}
      - LOG_TIMEZONE_OFFSET=${LOG_TIMEZONE_OFFSET:-480}
      - LOG_SUCCESS_REQUESTS=${LOG_SUCCESS_REQUESTS:-false}
      - LOG_SAMPLING_RATE=${LOG_SAMPLING_RATE:-0.1}
      - LOG_STDOUT=${LOG_STDOUT:-true}
      - LOG_FILE=${LOG_FILE:-true}
      - LOG_IGNORE_PATHS=${LOG_IGNORE_PATHS:-/api/v1/metrics,/api/v1/docs,/uploads}
      - LOG_USE_REDIS_BUFFER=${LOG_USE_REDIS_BUFFER:-false}
      - LOG_REDIS_MAX_SIZE=${LOG_REDIS_MAX_SIZE:-20000}
      - LOG_CONSUMER_BATCH_SIZE=${LOG_CONSUMER_BATCH_SIZE:-500}
      - LOG_CONSUMER_POLL_INTERVAL=${LOG_CONSUMER_POLL_INTERVAL:-500}
      - LOG_BATCH_SIZE=${LOG_BATCH_SIZE:-200}
      - LOG_BATCH_INTERVAL=${LOG_BATCH_INTERVAL:-300}
      - LOG_MAX_QUEUE_SIZE=${LOG_MAX_QUEUE_SIZE:-20000}
      - DATABASE_URL=postgresql://postgres:postgres@postgres:5432/game_ai_cs?schema=public
      # 数据库连接池配置
      - DB_CONNECTION_LIMIT=${DB_CONNECTION_LIMIT:-50}
      - DB_POOL_TIMEOUT=${DB_POOL_TIMEOUT:-20}
      - DB_CONNECT_TIMEOUT=${DB_CONNECT_TIMEOUT:-10}
      - DB_QUERY_TIMEOUT=${DB_QUERY_TIMEOUT:-30}
      - DB_STATEMENT_TIMEOUT=${DB_STATEMENT_TIMEOUT:-30000}
      - DB_IDLE_TIMEOUT=${DB_IDLE_TIMEOUT:-600}
      - DB_POOL_MONITORING=${DB_POOL_MONITORING:-true}
      - DB_POOL_LOG_LEVEL=${DB_POOL_LOG_LEVEL:-warn}
      - JWT_SECRET=${JWT_SECRET:-your-secret-key-change-in-production}
      - JWT_EXPIRES_IN=${JWT_EXPIRES_IN:-8h}
      - FRONTEND_URL=http://localhost:20101,http://localhost:20102
      # Dify AI 配置
      - DIFY_API_KEY=${DIFY_API_KEY:-app-pVK7Yup3O9DG6Zr1p8cSfe3p}
      - DIFY_BASE_URL=${DIFY_BASE_URL:-http://ai.sh7road.com/v1}
      - REDIS_URL=${REDIS_URL:-redis://redis:6379}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - ENCRYPTION_SECRET_KEY=${ENCRYPTION_SECRET_KEY:-default-secret-key-change-in-production-32-chars!!}
      # 百度翻译配置
      - BAIDU_TRANSLATE_APP_ID=${BAIDU_TRANSLATE_APP_ID:-20250311002299702}
      - BAIDU_TRANSLATE_SECRET=${BAIDU_TRANSLATE_SECRET:-H1dETwWWqk45uN2DzGxK}
    ports:
      - "21101:21101"
    volumes:
      - ./backend/uploads:/app/uploads
      - ./backend/logs:/app/logs
      - ./backend/prisma:/app/prisma
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - game-ai-cs-network
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "10"
        labels: "service=backend,env=production"

  admin-portal:
    build:
      context: ./frontend/admin-portal
      dockerfile: Dockerfile
    container_name: game-ai-cs-admin-portal
    ports:
      - "20101:20101"
    depends_on:
      - backend
    networks:
      - game-ai-cs-network
    restart: unless-stopped

  player-app:
    build:
      context: ./frontend/player-app
      dockerfile: Dockerfile
    container_name: game-ai-cs-player-app
    ports:
      - "20102:20102"
    depends_on:
      - backend
    networks:
      - game-ai-cs-network
    restart: unless-stopped

  # ==================== 监控服务 (可选) ====================
  # Monitoring stack disabled for fast release; uncomment to re-enable.

  # prometheus:
  #   image: prom/prometheus:latest
  #   container_name: game-ai-cs-prometheus
  #   profiles:
  #     - monitoring
  #   ports:
  #     - "23101:9090"
  #   volumes:
  #     - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
  #     - ./monitoring/prometheus/alert-rules.yml:/etc/prometheus/alert-rules.yml
  #     - prometheus_data:/prometheus
  #   command:
  #     - '--config.file=/etc/prometheus/prometheus.yml'
  #     - '--storage.tsdb.path=/prometheus'
  #     - '--web.enable-lifecycle'
  #     - '--web.enable-admin-api'
  #   networks:
  #     - game-ai-cs-network
  #   restart: unless-stopped

  # alertmanager:
  #   image: prom/alertmanager:latest
  #   container_name: game-ai-cs-alertmanager
  #   profiles:
  #     - monitoring
  #   ports:
  #     - "23104:9093"
  #   volumes:
  #     - ./monitoring/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml
  #     - alertmanager_data:/alertmanager
  #   command:
  #     - '--config.file=/etc/alertmanager/alertmanager.yml'
  #     - '--storage.path=/alertmanager'
  #   networks:
  #     - game-ai-cs-network
  #   restart: unless-stopped

  # loki:
  #   image: grafana/loki:2.9.0
  #   container_name: game-ai-cs-loki
  #   profiles:
  #     - monitoring
  #   ports:
  #     - "23102:3100"
  #   volumes:
  #     - ./monitoring/loki/loki-config.yml:/etc/loki/local-config.yaml
  #     - loki_data:/loki
  #   command: -config.file=/etc/loki/local-config.yaml
  #   networks:
  #     - game-ai-cs-network
  #   restart: unless-stopped

  # promtail:
  #   image: grafana/promtail:2.9.0
  #   container_name: game-ai-cs-promtail
  #   profiles:
  #     - monitoring
  #   volumes:
  #     - ./monitoring/promtail/promtail-config.yml:/etc/promtail/config.yml
  #     - ./backend/logs:/var/log/backend:ro
  #   command: -config.file=/etc/promtail/config.yml
  #   networks:
  #     - game-ai-cs-network
  #   depends_on:
  #     - loki
  #   restart: unless-stopped

  # jaeger:
  #   image: jaegertracing/all-in-one:1.52
  #   container_name: game-ai-cs-jaeger
  #   profiles:
  #     - monitoring
  #   ports:
  #     - "23105:16686"  # Jaeger UI
  #     - "6831:6831/udp"  # Jaeger agent (Thrift compact)
  #     - "14268:14268"  # Jaeger collector HTTP
  #     - "14269:14269"  # Jaeger admin/health
  #   environment:
  #     - COLLECTOR_OTLP_ENABLED=true
  #     - SPAN_STORAGE_TYPE=memory
  #     - MEMORY_MAX_TRACES=100000
  #   networks:
  #     - game-ai-cs-network
  #   restart: unless-stopped

  # grafana:
  #   image: grafana/grafana:latest
  #   container_name: game-ai-cs-grafana
  #   profiles:
  #     - monitoring
  #   ports:
  #     - "23103:3000"
  #   volumes:
  #     - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
  #     - grafana_data:/var/lib/grafana
  #   environment:
  #     - GF_SECURITY_ADMIN_PASSWORD=admin
  #     - GF_USERS_ALLOW_SIGN_UP=false
  #     - GF_FEATURE_TOGGLES_ENABLE=traceToMetrics
  #   networks:
  #     - game-ai-cs-network
  #   depends_on:
  #     - prometheus
  #     - loki
  #     - jaeger
  #   restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
  # prometheus_data:
  # grafana_data:
  # loki_data:
  # alertmanager_data:

networks:
  game-ai-cs-network:
    driver: bridge
